{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WK14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP: Word2Vec\n",
    "\n",
    "### One-Hot Encoding + Classification = Embedding\n",
    "\n",
    "https://medium.com/@patrykmwieczorek/mastering-nlp-with-pytorch-word2vec-60a54030c720\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "# TODO: Add Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -P ./data/text https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/datasets/text/dickinson.txt\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/datasets/text/rappers.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on colab\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "def get_tokenizer(param):\n",
    "  return nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextUtils():\n",
    "  stop_1000_url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "  stop_100_url  = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\"\n",
    "\n",
    "  stopwords_list = requests.get(stop_100_url).content\n",
    "  stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "  @staticmethod\n",
    "  def create_vocab(text, max_words=200_000):\n",
    "    # create one big string\n",
    "    if type(text) is list:\n",
    "      text = \" \".join(text)\n",
    "\n",
    "    # remove punctuation, whitespaces and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
    "\n",
    "    # tokenize words\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    words = tokenizer(\"\".join(text))[:max_words]\n",
    "\n",
    "    # remove repeated words\n",
    "    vocab = list(set(words))\n",
    "\n",
    "    return words, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSequenceDataset(Dataset):\n",
    "  def __init__(self, text, max_words=200_000, window=2, symmetric_context=True):\n",
    "    super().__init__()\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    self.window = window\n",
    "    self.symmetric_context = symmetric_context\n",
    "    self.words, self.vocab = TextUtils.create_vocab(text, max_words=max_words)\n",
    "    wtoi = {word: i for i, word in enumerate([\"<UNK>\"] + self.vocab)}\n",
    "    itow = {i: word for i, word in enumerate(wtoi)}\n",
    "\n",
    "    self.wtoi = defaultdict(int, wtoi)\n",
    "    self.itow = defaultdict(lambda: \"<UNK>\", itow)\n",
    "    print(f\"{len(self.wtoi)} words in vocab\")\n",
    "    print(f\"{len(self.words)} words in text\")\n",
    "\n",
    "  def encode_word(self, word, return_tensors=False):\n",
    "    widx = self.wtoi[word.lower()]\n",
    "    if not return_tensors:\n",
    "      return widx\n",
    "    else:\n",
    "      return Tensor([widx]).long().to(self.device)\n",
    "\n",
    "  def encode(self, words):\n",
    "    widx = [self.wtoi[w.lower()] for w in words]\n",
    "    return Tensor(widx).long().to(self.device)\n",
    "\n",
    "  def decode_word(self, idx):\n",
    "    if type(idx) is int:\n",
    "      return self.itow[idx]\n",
    "    else:\n",
    "      return self.itow[idx.item()]\n",
    "\n",
    "  def decode(self, idxs_t):\n",
    "    idxs = idxs_t.tolist()\n",
    "    return [self.itow[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(TextSequenceDataset):\n",
    "  def __init__(self, text, max_words=200_000, window=2, symmetric_context=True):\n",
    "    super().__init__(text, max_words, window, symmetric_context)\n",
    "    self.X, self.Y = self.create_dataset(self.words, self.wtoi, self.window, self.symmetric_context)\n",
    "    assert len(self.X) == len(self.Y)\n",
    "\n",
    "  def create_dataset(self, words, wtoi, window, symmetric_context):\n",
    "    stopwords = TextUtils.stopwords + [\"=\", \":\", \",\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\"]\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for i in range(0, len(words)):\n",
    "      minj = i - window if symmetric_context else i + 1\n",
    "      maxj = i + window\n",
    "      if words[i] in stopwords:\n",
    "        continue\n",
    "      center_word = wtoi[words[i].lower()]\n",
    "      for j in range(minj, maxj + 1):\n",
    "        if j == i or j < 0 or j > len(words) - 1 or words[j] in stopwords:\n",
    "          continue\n",
    "        context_word = wtoi[words[j].lower()]\n",
    "        xs.append(center_word)\n",
    "        ys.append(context_word)\n",
    "    return Tensor(xs).long().to(self.device), Tensor(ys).long().to(self.device)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if type(idx) is slice:\n",
    "      return list(zip(self.X[idx], self.Y[idx]))\n",
    "    return (self.X[idx], self.Y[idx])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim=128):\n",
    "    super().__init__()\n",
    "    self.center_embeds = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "    self.context_embeds = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    center_word = self.center_embeds(x)\n",
    "    scores = torch.matmul(center_word, self.context_embeds.weight.t())\n",
    "    return scores\n",
    "\n",
    "  def get_N_closest(self, x, N=5, metric=\"lnorm\"):\n",
    "    # get word vector\n",
    "    x = self.center_embeds(x)\n",
    "\n",
    "    # calculate similarity between x and all center vectors\n",
    "    if metric == \"sine\":\n",
    "      cos_sim = nn.CosineSimilarity()\n",
    "      similarities = cos_sim(x, self.center_embeds.weight).squeeze()\n",
    "      largest = True\n",
    "    elif metric == \"lnorm\":\n",
    "      similarities = torch.cdist(x, self.center_embeds.weight).squeeze()\n",
    "      largest = False\n",
    "\n",
    "    # return top-N similar words by embeddings\n",
    "    values, indices = torch.topk(similarities, k=N, largest=largest)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/text/dickinson.txt\", \"r\") as f:\n",
    "  dickinson_text = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv(\"./data/text/rappers.csv\")\n",
    "rapper_text = lyrics_df[\"lyric\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(text=dickinson_text, max_words=500_000, window=3, symmetric_context=False)\n",
    "train_dl = DataLoader(dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SkipGram(vocab_size=len(dataset.wtoi), embed_dim=64).to(mdevice)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "ctr,ctx = next(iter(train_dl))\n",
    "print(ctr.shape, ctx.shape)\n",
    "\n",
    "ctx_pred = model(ctr)\n",
    "print(ctx_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for center, context in train_dl:\n",
    "    optim.zero_grad()\n",
    "    context_pred = model(center)\n",
    "    loss = loss_fn(context_pred, context)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset.encode_word(\"wild\", return_tensors=True)\n",
    "\n",
    "top5s = model.get_N_closest(query, N=5, metric=\"sine\")\n",
    "top5l = model.get_N_closest(query, N=5, metric=\"lnorm\")\n",
    "\n",
    "print(dataset.decode(top5s))\n",
    "print(dataset.decode(top5l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : Translate ...\n",
    "\n",
    "Get Dickinson phrase -> list of embeddings.\n",
    "\n",
    "Get start word from rappers, follow Dickinson directions and get nearest word(s)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "\n",
    "### Classification + Classification + Classification + ...\n",
    "\n",
    "- Output of NN becomes an input for next prediction\n",
    "\n",
    "# TODO: Images\n",
    "\n",
    "https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/\n",
    "\n",
    "https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "\n",
    "https://medium.com/@prudhviraju.srivatsavaya/lstm-vs-gru-c1209b8ecb5a\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(TextSequenceDataset):\n",
    "  def __init__(self, text, max_words=200_000, window=2):\n",
    "    super().__init__(text, max_words, window, symmetric_context=False)\n",
    "    self.words_t = self.encode(self.words)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.words) - self.window\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    target = self.words_t[idx + self.window]\n",
    "    context = self.words_t[idx : idx + self.window]\n",
    "    return context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordGRU(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim=64, hidden_dim=256, num_layers=2):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    out, hidden = self.gru(x, hidden)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NGramDataset(text=dickinson_text, max_words=500_000, window=5)\n",
    "train_dl = DataLoader(dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = NextWordGRU(vocab_size=len(dataset.wtoi), embedding_dim=64, hidden_dim=256, num_layers=2).to(mdevice)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "input, target = next(iter(train_dl))\n",
    "print(input.shape, target.shape)\n",
    "\n",
    "output, hidden = model(input, None)\n",
    "print(output.shape, hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for input, target in train_dl:\n",
    "    optim.zero_grad()\n",
    "    hidden = None\n",
    "    output, hidden = model(input, hidden)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset.encode([\"you\"]).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  output, _ = model(query, None)\n",
    "  output = output.squeeze()\n",
    "  top1 = output.argmax()\n",
    "  top5 = output.argsort(descending=True)[:5]\n",
    "  print(dataset.decode_word(top1))\n",
    "  print(dataset.decode(top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "phrase = [\"You\"]\n",
    "hidden = None\n",
    "num_candidates = 2\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for w in range(10):\n",
    "    query = dataset.encode(phrase).unsqueeze(0)\n",
    "    output, hidden = model(query, None)\n",
    "    output = output.squeeze()\n",
    "    candidates = dataset.decode(output.argsort(descending=True)[:num_candidates])\n",
    "    phrase.append(choice(candidates))\n",
    "\n",
    "print(\" \".join(phrase))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
