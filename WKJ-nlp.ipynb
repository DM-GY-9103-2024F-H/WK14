{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WKJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP: Word2Vec\n",
    "\n",
    "### Encoding + Classification = Embedding\n",
    "\n",
    "https://medium.com/@patrykmwieczorek/mastering-nlp-with-pytorch-word2vec-60a54030c720\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "\n",
    "# TODO: Add Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -P ./data/text https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/datasets/text/dickinson.txt\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/datasets/text/rappers.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import string\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextUtils():\n",
    "\tstop_1000_url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "\tstop_100_url  = \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\"\n",
    "\n",
    "\tstopwords_list = requests.get(stop_100_url).content\n",
    "\tstopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef create_vocab(text, max_words=200_000):\n",
    "\t\t# create one big string\n",
    "\t\tif type(text) is list:\n",
    "\t\t\ttext = \" \".join(text)\n",
    "\n",
    "\t\t# remove punctuation, whitespaces and convert to lowercase\n",
    "\t\ttext = text.translate(str.maketrans('', '', string.punctuation)).lower().strip()\n",
    "\n",
    "\t\t# tokenize words\n",
    "\t\ttokenizer = get_tokenizer(\"basic_english\")\n",
    "\t\twords = tokenizer(\"\".join(text))[:max_words]\n",
    "\n",
    "\t\t# remove repeated words\n",
    "\t\tvocab = list(set(words))\n",
    "\n",
    "\t\treturn words, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSequenceDataset(Dataset):\n",
    "\tdef __init__(self, text, max_words=200_000, window=2, symmetric_context=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\t\tself.window = window\n",
    "\t\tself.symmetric_context = symmetric_context\n",
    "\t\tself.words, self.vocab = TextUtils.create_vocab(text, max_words=max_words)\n",
    "\t\twtoi = {word: i for i, word in enumerate([\"<UNK>\"] + self.vocab)}\n",
    "\t\titow = {i: word for i, word in enumerate(wtoi)}\n",
    "\n",
    "\t\tself.wtoi = defaultdict(int, wtoi)\n",
    "\t\tself.itow = defaultdict(lambda: \"<UNK>\", itow)\n",
    "\t\tprint(f\"{len(self.wtoi)} words in vocab\")\n",
    "\t\tprint(f\"{len(self.words)} words in text\")\n",
    "\n",
    "\tdef encode_word(self, word, return_tensors=False):\n",
    "\t\twidx = self.wtoi[word]\n",
    "\t\tif not return_tensors:\n",
    "\t\t\treturn widx\n",
    "\t\telse:\n",
    "\t\t\treturn Tensor([widx]).long().to(self.device)\n",
    "\n",
    "\tdef encode(self, words):\n",
    "\t\twidx = [self.wtoi[w] for w in words]\n",
    "\t\treturn Tensor(widx).long().to(self.device)\n",
    "\n",
    "\tdef decode_word(self, idx):\n",
    "\t\tif type(idx) is int:\n",
    "\t\t\treturn self.itow[idx]\n",
    "\t\telse:\n",
    "\t\t\treturn self.itow[idx.item()]\n",
    "\n",
    "\tdef decode(self, idxs_t):\n",
    "\t\tidxs = idxs_t.tolist()\n",
    "\t\treturn [self.itow[i] for i in idxs]\n",
    "\n",
    "class SkipGramDataset(TextSequenceDataset):\n",
    "\tdef __init__(self, text, max_words=200_000, window=2, symmetric_context=True):\n",
    "\t\tsuper().__init__(text, max_words, window, symmetric_context)\n",
    "\t\tself.X, self.Y = self.create_dataset(self.words, self.wtoi, self.window, self.symmetric_context)\n",
    "\t\tassert len(self.X) == len(self.Y)\n",
    "\n",
    "\tdef create_dataset(self, words, wtoi, window, symmetric_context):\n",
    "\t\tstopwords = TextUtils.stopwords + [\"=\", \":\", \",\", \"(\", \")\", \"{\", \"}\", \"[\", \"]\"]\n",
    "\t\txs, ys = [], []\n",
    "\n",
    "\t\tfor i in range(0, len(words)):\n",
    "\t\t\tminj = i - window if symmetric_context else i + 1\n",
    "\t\t\tmaxj = i + window\n",
    "\t\t\tif words[i] in stopwords:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tcenter_word = wtoi[words[i]]\n",
    "\t\t\tfor j in range(minj, maxj + 1):\n",
    "\t\t\t\tif j == i or j < 0 or j > len(words) - 1 or words[j] in stopwords:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tcontext_word = wtoi[words[j]]\n",
    "\t\t\t\txs.append(center_word)\n",
    "\t\t\t\tys.append(context_word)\n",
    "\t\treturn Tensor(xs).long().to(self.device), Tensor(ys).long().to(self.device)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif type(idx) is slice:\n",
    "\t\t\treturn list(zip(self.X[idx], self.Y[idx]))\n",
    "\t\treturn (self.X[idx], self.Y[idx])\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_dim=128):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.center_embeds = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\t\tself.context_embeds = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tcenter_word = self.center_embeds(x)\n",
    "\t\tscores = torch.matmul(center_word, self.context_embeds.weight.t())\n",
    "\t\treturn scores\n",
    "\n",
    "\tdef get_N_closest(self, x, N=5, metric=\"lnorm\"):\n",
    "\t\t# get word vector\n",
    "\t\tx = self.center_embeds(x)\n",
    "\n",
    "\t\t# calculate similarity between x and all center vectors\n",
    "\t\tif metric == \"sine\":\n",
    "\t\t\tcos_sim = nn.CosineSimilarity()\n",
    "\t\t\tsimilarities = cos_sim(x, self.center_embeds.weight).squeeze()\n",
    "\t\t\tlargest = True\n",
    "\t\telif metric == \"lnorm\":\n",
    "\t\t\tsimilarities = torch.cdist(x, self.center_embeds.weight).squeeze()\n",
    "\t\t\tlargest = False\n",
    "\n",
    "\t\t# return top-N similar words by embeddings\n",
    "\t\tvalues, indices = torch.topk(similarities, k=N, largest=largest)\n",
    "\t\treturn indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/text/dickinson.txt\", \"r\") as f:\n",
    "  dickinson_text = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_csv(\"./data/text/rappers.csv\")\n",
    "rapper_text = lyrics_df[\"lyric\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(text=dickinson_text, max_words=500_000, window=3, symmetric_context=False)\n",
    "train_dl = DataLoader(dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SkipGram(vocab_size=len(dataset.wtoi), embed_dim=64).to(mdevice)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "ctr,ctx = next(iter(train_dl))\n",
    "print(ctr.shape, ctx.shape)\n",
    "\n",
    "ctx_pred = model(ctr)\n",
    "print(ctx_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for center, context in train_dl:\n",
    "    optim.zero_grad()\n",
    "    context_pred = model(center)\n",
    "    loss = loss_fn(context_pred, context)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset.encode_word(\"wild\", return_tensors=True)\n",
    "\n",
    "top5s = model.get_N_closest(query, N=5, metric=\"sine\")\n",
    "top5l = model.get_N_closest(query, N=5, metric=\"lnorm\")\n",
    "\n",
    "print(dataset.decode(top5s))\n",
    "print(dataset.decode(top5l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : Translate ...\n",
    "\n",
    "Get Dickinson phrase -> list of embeddings.\n",
    "\n",
    "Get start word from rappers, follow directions from embeddings, get nearest word(s)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs\n",
    "\n",
    "### Classification + Classification + Classification + ...\n",
    "\n",
    "- Output of NN becomes an input for next prediction\n",
    "\n",
    "# TODO: Images\n",
    "\n",
    "https://machinelearningmastery.com/an-introduction-to-recurrent-neural-networks-and-the-math-that-powers-them/\n",
    "\n",
    "https://machinelearningmastery.com/models-sequence-prediction-recurrent-neural-networks/\n",
    "\n",
    "https://medium.com/@prudhviraju.srivatsavaya/lstm-vs-gru-c1209b8ecb5a\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(TextSequenceDataset):\n",
    "\tdef __init__(self, text, max_words=200_000, window=2):\n",
    "\t\tsuper().__init__(text, max_words, window, symmetric_context=False)\n",
    "\t\tself.words_t = self.encode(self.words)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.words) - self.window\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\ttarget = self.words_t[idx + self.window]\n",
    "\t\tcontext = self.words_t[idx : idx + self.window]\n",
    "\t\treturn context, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordGRU(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim=64, hidden_dim=256, num_layers=2):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    out, hidden = self.gru(x, hidden)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NGramDataset(text=dickinson_text, max_words=500_000, window=3)\n",
    "train_dl = DataLoader(dataset, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = NextWordGRU(vocab_size=len(dataset.wtoi), embedding_dim=64, hidden_dim=256, num_layers=2).to(mdevice)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "input, target = next(iter(train_dl))\n",
    "print(input.shape, target.shape)\n",
    "\n",
    "output, hidden = model(input, None)\n",
    "print(output.shape, hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for input, target in train_dl:\n",
    "    optim.zero_grad()\n",
    "    hidden = None\n",
    "    output, hidden = model(input, hidden)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset.encode([\"not\",\"one\",\"more\"]).unsqueeze(0)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  output, _ = model(query, None)\n",
    "  output = output.squeeze()\n",
    "  top1 = output.argmax()\n",
    "  top5 = output.argsort(descending=True)[:5]\n",
    "  print(dataset.decode_word(top1))\n",
    "  print(dataset.decode(top5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
